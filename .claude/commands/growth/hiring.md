---
name: growth-operator-hiring
description: Use when hiring growth operators, evaluating growth candidates, designing growth hiring processes, or creating interview rubrics for product/growth/ops roles
---

# Full-Loop Growth Operator Hiring

## Overview

This skill provides a framework for sourcing, vetting, and hiring "full-loop" growth operators - people with **π-shaped skills** who own designated growth loops end-to-end. It replaces functional hiring ("we need a paid marketer") with capability-based hiring ("we need someone to own our activation loop").

**Core principle:** Hire for sector/business model expertise + meta-skills, not functional expertise. A growth operator who deeply understands B2B SaaS subscription dynamics will outperform a "growth expert" who's only worked in e-commerce.

## The π-Shaped Skills Model

Traditional hiring looks for T-shaped skills (deep in one area). Growth operators need **π-shaped skills**:

```
     Broad knowledge across product, marketing, ops
     ─────────────────────────────────────────────
              │                    │
              │                    │
         Deep in            Deep in
         Area 1             Area 2
    (e.g., analytics)  (e.g., lifecycle)
```

**What to look for:**
- **Breadth:** Can navigate product, growth/marketing, and ops conversations
- **Depth:** Has 2+ areas of genuine expertise (not just "familiar with")
- **Sector fit:** Understands YOUR business model dynamics (B2B SaaS ≠ B2C consumer ≠ marketplace)

---

## Scoring System: 0-4 Scale with Behavioral Anchors

All evaluations use this scale. **Behavioral anchors are required** - no vibes-based scoring.

| Score | Label | Meaning |
|-------|-------|---------|
| 0 | No evidence | Cannot demonstrate the competency |
| 1 | Emerging | Shows awareness but no practical application |
| 2 | Developing | Has applied it but with gaps or inconsistency |
| 3 | Proficient | Consistently demonstrates; can teach others |
| 4 | Expert | Exceptional; has innovated or achieved outsized results |

**Scoring discipline:** If you can't point to specific evidence, the score is 0-1.

---

## Weighting Model

| Category | Weight | What It Includes |
|----------|--------|------------------|
| Sector/Business Model Expertise | 40% | Domain pattern recognition, relevant experience, business model fluency |
| Meta-Skills (Mindset) | 35% | Agency, ownership, learning velocity, mistake recovery, curiosity |
| Execution Skills | 25% | Analytical rigor, product sense, AI fluency |

**Why this weighting:** Sector expertise is weighted heavily because growth operators must hit the ground running. They can't learn "how B2B SaaS works" on the job. Meta-skills determine long-term trajectory but need a foundation to apply against.

---

## 1. Role Definition Framework

Before sourcing, define what they'll actually own. Use this template:

### Role Scope Template

```markdown
## [Role Title]

### The Loop They Own
[Describe the specific growth loop or motion. Be concrete.]
Example: "Owns the email → engagement → re-engagement loop for churned users"

### Primary Metrics
- North star: [e.g., Reactivated users per month]
- Driver metrics: [e.g., Open rate, click rate, return visit rate]
- Guardrails: [e.g., Unsubscribe rate, spam complaints]

### Sector/Model Requirements
- [ ] Must have: [e.g., B2B SaaS experience, PLG model familiarity]
- [ ] Prefer: [e.g., Healthcare/regulated industry, >50 employee companies]
- [ ] Anti-signal: [e.g., Only consumer experience, only paid acquisition]

### Interfaces
- Works with: [Product, Analytics, Customer Success, etc.]
- Reports to: [Title]
- Decision rights: [What can they decide unilaterally vs. needs approval]

### 90-Day Success Criteria
- [ ] [Concrete deliverable 1]
- [ ] [Concrete deliverable 2]
- [ ] [Metric improvement target]
```

---

## 2. Sourcing Playbook

### Signal-Rich Channels (ranked by signal quality)

| Channel | Signal Quality | Why |
|---------|----------------|-----|
| Warm referrals from operators | Highest | Pre-vetted for quality; understand what "good" looks like |
| Reforge / Lenny's / OnDeck alumni | High | Self-selected for growth mindset; paid to learn |
| Newsletter/content creators in space | High | Demonstrates thinking publicly; can assess quality directly |
| Open-source/side project contributors | Medium-High | Shows builder mentality; evidence of shipping |
| LinkedIn (targeted search) | Medium | High volume, lower signal; requires careful filtering |
| Job boards | Low | Spray-and-pray applications; mostly noise |

### LinkedIn Profile Pattern Recognition

**Green Flags:**
- Progression within companies (not just job-hopping for titles)
- Specific metrics in role descriptions ("grew X from A to B")
- Evidence of cross-functional work
- Content/posts showing how they think
- Side projects, newsletters, or public work

**Red Flags:**
- Only worked at large/late-stage companies
- Vague descriptions ("responsible for growth")
- Only one function across all roles (e.g., only email marketing)
- No evidence of learning or curiosity
- Title inflation without scope to match

### Referral Ask Template

> "I'm looking for a growth operator who can own [specific loop/motion] at [company]. Someone who's run end-to-end experiments, not just executed playbooks. Strong preference for [sector/model] experience. Who's the best person you've worked with who fits this?"

---

## 3. Initial Screening Rubric

### Resume/Portfolio Quick Scoring

Score each dimension 0-4:

| Dimension | What to Look For | Score |
|-----------|------------------|-------|
| **End-to-end ownership evidence** | Did they own loops, or just execute tactics? Look for "built X from 0" language | __ |
| **Growth mindset signals** | Learning, iteration, admitting failures, curiosity indicators | __ |
| **AI tool adoption signals** | Mentions of AI tools, automation, modern stack | __ |
| **Sector/model fit** | Experience in similar business models, industries | __ |

**Quick pass/fail:** If Sector/model fit < 2 AND End-to-end ownership < 2, pass.

### 15-Minute Phone Screen Questions

1. "Walk me through the growth loop you owned most recently. What was broken, and what did you do?"
   - Listen for: Specificity, ownership language, metric awareness

2. "Tell me about an experiment that failed. What did you learn?"
   - Listen for: Intellectual honesty, learning orientation, not defensive

3. "How do you decide what to work on when everything seems important?"
   - Listen for: Prioritization framework, impact thinking, not just "manager told me"

**Pass/Fail:** Must demonstrate ownership language and learning orientation to advance.

---

## 4. Deep Evaluation Framework

### Stage A: Case Study / Work Sample

Use this templated "growth loop diagnosis" framework. Customize the specifics to your company.

#### Case Study Template

```markdown
## Growth Loop Diagnosis

### Situation
[Describe your company/product briefly - 2-3 sentences]

### The Problem
[Describe a real or realistic growth challenge. Include specific numbers.]
Example: "Our trial-to-paid conversion dropped from 12% to 8% over the last quarter.
We send 3 onboarding emails. Open rates are 45%, 32%, 22%. Click rates are 8%, 5%, 3%."

### Data Provided
[Attach or describe the data they can analyze - keep it to 1-2 data sources]
- Cohort conversion data by signup source
- Email engagement metrics by segment

### Constraints
- Engineering capacity: 2 weeks of dev time available
- Timeline: Need to show improvement within 30 days
- Budget: $5K for any external tools/services

### Deliverable (60-90 minutes)
1. What's your hypothesis for why conversion dropped?
2. What would you test first, and why that over alternatives?
3. How would you measure success?
4. What would you need to know that isn't in the data provided?
```

#### Case Study Scoring Rubric

| Dimension | 0 | 1 | 2 | 3 | 4 |
|-----------|---|---|---|---|---|
| **Hypothesis quality** | No clear hypothesis | Vague hypothesis | Reasonable hypothesis, weak reasoning | Clear hypothesis with supporting logic | Multiple hypotheses prioritized with strong reasoning |
| **Analytical rigor** | Doesn't engage with data | Mentions data superficially | Uses data but misses key insights | Solid analysis, catches important patterns | Deep analysis, surfaces non-obvious insights |
| **Prioritization** | Random selection | Some logic but weak | Reasonable framework (impact × effort) | Strong framework, defends tradeoffs | Exceptional prioritization with constraints awareness |
| **Measurement plan** | No plan | Vague plan | Has metrics but no clear success criteria | Clear metrics + success criteria | Metrics + holdout/control considerations |
| **Curiosity / questions** | No questions | Surface questions | Good questions about data | Probing questions that change analysis | Questions that reveal deeper thinking |

**Minimum to advance:** Average ≥ 2.5 across dimensions.

---

### Stage B: Behavioral Interview

Questions organized by trait. Use 2-3 questions per trait based on role priorities.

#### Agency / Ownership Questions

| Question | What 3-4 sounds like | What 0-1 sounds like |
|----------|---------------------|---------------------|
| "Tell me about a time you saw a problem no one asked you to solve. What did you do?" | Specific problem, took initiative without permission, drove to outcome | Waited for direction, or vague/generic answer |
| "Describe a situation where you had to make a decision without enough information." | Made a call, explains reasoning, adjusted when learned more | Paralyzed, escalated everything, or made decision without reflection |
| "When was the last time you disagreed with your manager about priorities? How did you handle it?" | Advocated with data/logic, accepted decision either way, didn't undermine | Complied silently, complained to others, or couldn't recall disagreeing |

#### Learning Velocity Questions

| Question | What 3-4 sounds like | What 0-1 sounds like |
|----------|---------------------|---------------------|
| "What's something you learned in the last month that changed how you work?" | Specific, recent, applied the learning | Can't recall, or learning wasn't applied |
| "Tell me about the last skill you taught yourself. How?" | Clear learning process, resources used, can demonstrate competence | Vague, or relied entirely on formal training |
| "Describe an experiment that gave you unexpected results. What did you conclude?" | Changed mental model, not just tactics | Didn't update beliefs, or explained away the data |

#### Mistake Recovery Questions

| Question | What 3-4 sounds like | What 0-1 sounds like |
|----------|---------------------|---------------------|
| "Tell me about a significant mistake you made at work. What happened after?" | Owns it fully, explains what they learned, shows changed behavior | Blames others, minimizes, or can't recall mistakes |
| "Describe a project that failed. What would you do differently?" | Identifies root cause (not just symptoms), specific changes | Vague lessons, or "I'd try harder" |
| "When was the last time you changed your mind about something important?" | Recent, specific, explains what evidence changed their view | Can't recall, or changes feel superficial |

#### Prioritization Questions

| Question | What 3-4 sounds like | What 0-1 sounds like |
|----------|---------------------|---------------------|
| "How do you decide what to work on when everything seems urgent?" | Has framework (impact × effort, ICE, etc.), can give examples | No framework, reacts to loudest voice |
| "Tell me about a time you said no to a stakeholder request. How?" | Clear reasoning, communicated tradeoffs, maintained relationship | Always says yes, or can't recall saying no |
| "Describe a situation where you had to cut scope. What did you cut and why?" | Principled cuts (not random), defended the decision | Cut randomly, or never cut scope |

#### Curiosity / Reinvention Questions

| Question | What 3-4 sounds like | What 0-1 sounds like |
|----------|---------------------|---------------------|
| "What's something about your field that most people get wrong?" | Has a contrarian view they can defend | Conventional wisdom, no unique perspective |
| "How do you stay current in growth/product?" | Specific sources, active learning, can cite recent insights | "I read stuff" without specifics |
| "If you had to rebuild your skill set from scratch, what would you learn first?" | Thoughtful prioritization, explains why | Can't articulate, or lists trendy things without reasoning |

#### Behavioral Interview Scoring

For each trait, score 0-4 based on the quality of responses across 2-3 questions.

| Trait | Score | Evidence/Notes |
|-------|-------|----------------|
| Agency / Ownership | __ | |
| Learning Velocity | __ | |
| Mistake Recovery | __ | |
| Prioritization | __ | |
| Curiosity / Reinvention | __ | |
| **Meta-Skills Average** | __ | |

---

### Stage C: AI Fluency Assessment (Two-Stage)

#### Part 1: Application Artifact (Gate Before Interview)

**Prompt to candidate:**

> "Submit a recording or transcript of a chat-based building session you're proud of. This should be a real session where you used AI tools (Claude, ChatGPT, etc.) to solve a growth, product, or analysis problem.
>
> We're looking for: problem framing, tool selection, iteration quality, judgment about when to trust/distrust output, and the outcome.
>
> Length: 15-30 minutes of session time, or equivalent transcript."

#### Artifact Evaluation Rubric

| Dimension | 0 | 1 | 2 | 3 | 4 |
|-----------|---|---|---|---|---|
| **Problem framing** | Unclear what they're solving | Vague problem statement | Clear problem, basic framing | Well-structured problem with constraints | Exceptional framing, anticipates edge cases |
| **Tool selection** | Wrong tool for the job | Reasonable but suboptimal | Good match of tool to problem | Strategic tool selection with reasoning | Excellent tool orchestration |
| **Iteration quality** | No iteration; accepts first output | Minimal iteration | Reasonable iteration on prompts | Strong iteration with clear improvement | Masterful iteration showing deep fluency |
| **Judgment** | Takes output at face value | Some skepticism but misses issues | Catches obvious errors | Good quality control, validates claims | Exceptional judgment, knows limits precisely |
| **Outcome** | No clear outcome | Weak outcome | Reasonable result | Strong result with clear value | Exceptional outcome, demonstrates impact |

**Pass/fail gate:** Average ≥ 2.0 to advance to live exercise. If <2.0, no live interview.

#### Part 2: Live Exercise (Interview Stage)

**Format:** 45-minute live problem-solving using META PM framework.

**META PM Framework - Three Pillars:**
1. **Analytical Rigor:** Can they break down problems systematically?
2. **Product Sense:** Can they anticipate what's impactful AND feasible?
3. **AI Fluency:** Do they use AI strategically, not just tactically?

**Exercise setup:**

> "I'm going to give you a growth problem. You have access to [Claude/ChatGPT]. Solve it live - I want to see how you work, not just the answer. Talk through your thinking."

**Example problems (pick one):**
- "Our onboarding completion rate dropped 15% last month. Here's a data export. Diagnose the problem and propose three experiments."
- "We need to increase email engagement without increasing send volume. Design the experiment and measurement plan."
- "Users who do [action X] retain 3x better. Design a growth loop to increase [action X] without hurting activation."

#### Live Exercise Scoring

| Dimension | What to Observe | Score |
|-----------|-----------------|-------|
| **Tool selection** | Do they pick the right tool for the problem? Switch when needed? | __ |
| **Prompt quality** | Clear context, specific asks, iterates effectively | __ |
| **Iteration** | Improves outputs, doesn't accept bad results | __ |
| **When to pivot** | Knows when AI isn't helping, tries different approach | __ |
| **Analytical rigor** | Systematic thinking, doesn't skip steps | __ |
| **Product sense** | Anticipates feasibility, user impact, implementation | __ |
| **Outcome quality** | Did they actually solve the problem? | __ |

---

## 5. Final Evaluation & Decision

### Aggregate Scoring Model

Use this scorecard to compile all evaluations:

| Category | Weight | Components | Raw Score (avg) | Weighted |
|----------|--------|------------|-----------------|----------|
| Sector/Business Model Expertise | 40% | Resume fit + Case study context understanding | __ | __ |
| Meta-Skills (Mindset) | 35% | Behavioral interview average | __ | __ |
| Execution Skills | 25% | Case study score + AI assessment average | __ | __ |
| **TOTAL** | 100% | | | __ |

### Decision Thresholds

| Weighted Total | Decision |
|----------------|----------|
| ≥ 3.2 | **Strong hire** - make offer |
| 2.8 - 3.1 | **Hire** - make offer, note development areas |
| 2.4 - 2.7 | **Borderline** - only if exceptional in one dimension AND no red flags |
| < 2.4 | **No hire** - do not make offer |

### Red Flags (Disqualifying Regardless of Score)

- Cannot own mistakes or blames others
- No evidence of learning from failures
- Only tactical execution, no strategic thinking
- Cannot articulate why something worked (just "we tried things")
- Defensive about feedback or pushback
- No questions for interviewer (lack of curiosity)

### Reference Check Questions (Trait-Focused)

Don't verify resume - verify traits:

1. "How did [candidate] handle a situation where they were wrong?"
2. "Tell me about a time [candidate] took initiative without being asked."
3. "What's something [candidate] learned on the job that changed how they worked?"
4. "How did [candidate] prioritize when there were competing demands?"
5. "Would you work with [candidate] again? Why or why not?"

---

## 6. Templates & Tools

### Candidate Comparison Scorecard

```markdown
## Candidate Comparison: [Role]

| Dimension | Candidate A | Candidate B | Candidate C |
|-----------|-------------|-------------|-------------|
| **Sector Expertise (40%)** | | | |
| Resume/portfolio fit | /4 | /4 | /4 |
| Case study context | /4 | /4 | /4 |
| **Meta-Skills (35%)** | | | |
| Agency/Ownership | /4 | /4 | /4 |
| Learning Velocity | /4 | /4 | /4 |
| Mistake Recovery | /4 | /4 | /4 |
| Prioritization | /4 | /4 | /4 |
| Curiosity | /4 | /4 | /4 |
| **Execution Skills (25%)** | | | |
| Case study avg | /4 | /4 | /4 |
| AI artifact | /4 | /4 | /4 |
| AI live exercise | /4 | /4 | /4 |
| **Weighted Total** | | | |
| **Decision** | | | |
| **Notes** | | | |
```

### Interview Debrief Template

```markdown
## Interview Debrief: [Candidate Name]

**Interviewer:**
**Stage:** [Screen / Case Study / Behavioral / AI Assessment]
**Date:**

### Summary (1-2 sentences)

### Scores
[Paste relevant scoring section]

### Key Evidence
- Strongest signal:
- Concerning signal:

### Recommendation
[ ] Strong hire  [ ] Hire  [ ] Borderline  [ ] No hire

### Questions for Next Round (if advancing)
```

---

## Common Mistakes

| Mistake | Fix |
|---------|-----|
| Scoring on vibes, not evidence | Use behavioral anchors; if no evidence, score 0-1 |
| Weighting "culture fit" too heavily | Culture fit is table stakes, not a dimension to score |
| Skipping AI assessment | AI fluency is non-negotiable for modern growth work |
| Only one interviewer per stage | Two interviewers minimum for calibration |
| Not calibrating scores across interviewers | Debrief before making decisions; align on anchors |
| Hiring for functional expertise over sector fit | A "growth expert" from wrong sector will underperform |
| Accepting "I was part of a team that..." | Push for individual contribution and ownership |
